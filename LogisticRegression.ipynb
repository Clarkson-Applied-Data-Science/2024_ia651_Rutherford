{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, Value, Features\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModel\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torch.nn.functional import cross_entropy\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_csv('Data\\\\twitter_training.csv')\n",
    "vdf = pd.read_csv('Data\\\\twitter_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicList = []\n",
    "Sentimentlist = []\n",
    "commentstring = \"\"\n",
    "\n",
    "for x in ntdf.iterrows():\n",
    "    TopicList.append(x[1][\"Topic\"])\n",
    "    Sentimentlist.append(x[1][\"Sentiment\"])\n",
    "    commentstring = commentstring + str(x[1][\"Comment\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cTopiclist = Counter(TopicList)\n",
    "cSentimentlist = Counter(Sentimentlist)\n",
    "commentwordlist = commentstring.split(\" \")\n",
    "ccommentwordlist = Counter(commentwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cTopiclist.most_common(10))\n",
    "print(cSentimentlist.most_common(4))\n",
    "print(ccommentwordlist.most_common(10))\n",
    "print(len(set(ccommentwordlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleaning(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    x = str(x)\n",
    "    na = nltk.regexp_tokenize(x.lower(), r'(\\b[\\w]{2,}\\b)')\n",
    "    naa = []\n",
    "    for z in na:\n",
    "        naa.append(lemmatizer.lemmatize(z))\n",
    "    nnaa = \" \".join(naa)\n",
    "    return nnaa\n",
    "\n",
    "def RSW(x):\n",
    "    x = str(x)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = nltk.regexp_tokenize(x.lower(), r'(\\b[\\w]{2,}\\b)')\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    nt = \" \".join(filtered_sentence)\n",
    "    return nt\n",
    "\n",
    "#Creating Lem With Stop\n",
    "for index, row in tdf.iterrows():\n",
    "    oa = row[\"Comment\"]\n",
    "    g = row.name\n",
    "    tdf.loc[g, \"clean_Comment\"] = Cleaning(oa)\n",
    "\n",
    "#Creating No Lem With out Stop\n",
    "for index, row in tdf.iterrows():\n",
    "    oa = row[\"Comment\"]\n",
    "    g = row.name\n",
    "    tdf.loc[g, \"No_Stop_Words_Comment\"] = RSW(oa)\n",
    "#Creating Lem With Out Stop\n",
    "for index, row in tdf.iterrows():\n",
    "    oa = row[\"Comment\"]\n",
    "    g = row.name\n",
    "    ns = Cleaning(oa)\n",
    "    nns = RSW(ns)\n",
    "    tdf.loc[g, \"No_Stop_Words_Lemmatized_Comment\"] = nns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tdf, tdf[\"Sentiment\"], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TheOne(typeofabstract, binary, ngram):\n",
    "            pipeline = Pipeline([(\"Vec\", CountVectorizer(binary= binary, ngram_range=(1, ngram))), (\"Clf\", LogisticRegression(max_iter= 3000))])\n",
    "            scores = cross_validate(pipeline,\n",
    "                X_train[typeofabstract],\n",
    "                y_train,\n",
    "                cv=10, \n",
    "                scoring=['f1_macro'])\n",
    "            pipeline.fit(X_train[typeofabstract], y_train)\n",
    "            y_true = list(X_test[\"Comment\"])\n",
    "            y_pred = pipeline.predict(X_test[typeofabstract])\n",
    "            stufflist = []\n",
    "            stufflist.append(typeofabstract)\n",
    "            if binary == True:\n",
    "                stufflist.append(\"Binary\")\n",
    "            else:\n",
    "                stufflist.append(\"Non-Binary\") \n",
    "            if ngram == 1:\n",
    "                stufflist.append(\"1_ngram\") \n",
    "            else:\n",
    "                stufflist.append(\"2_ngram\")\n",
    "            return metrics.f1_score(y_true, y_pred, average='macro'), scores, stufflist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tdf.iterrows():\n",
    "    if row[\"Comment\"] == float:\n",
    "        tdf.drop(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['Comment'] = tdf['Comment'].astype(str)\n",
    "tdf['clean_Comment'] = tdf['clean_Comment'].astype(str)\n",
    "tdf['No_Stop_Words_Comment'] = tdf['No_Stop_Words_Comment'].astype(str)\n",
    "tdf['No_Stop_Words_Lemmatized_Comment'] = tdf['No_Stop_Words_Lemmatized_Comment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f1_list = []\n",
    "cv_f1_list = []\n",
    "cv_f1t_list = []\n",
    "event_list = []\n",
    "a, b, c = TheOne(\"clean_Comment\", True, 1)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"1\")\n",
    "a, b, c = TheOne(\"No_Stop_Words_Comment\", True, 1)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"2\")\n",
    "a, b, c = TheOne(\"No_Stop_Words_Lemmatized_Comment\", True, 1)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"3\")\n",
    "a, b, c = TheOne(\"Comment\", True, 1)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"4\")\n",
    "a, b, c = TheOne(\"clean_Comment\", False, 1)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"5\")\n",
    "a, b, c = TheOne(\"No_Stop_Words_Comment\", False, 1)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"6\")\n",
    "a, b, c = TheOne(\"No_Stop_Words_Lemmatized_Comment\", False, 1)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"7\")\n",
    "a, b, c = TheOne(\"Comment\", False, 1)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"8\")\n",
    "a, b, c = TheOne(\"clean_Comment\", True, 2)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"9\")\n",
    "a, b, c = TheOne(\"No_Stop_Words_Comment\", True, 2)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"10\")\n",
    "a, b, c = TheOne(\"No_Stop_Words_Lemmatized_Comment\", True, 2)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"11\")\n",
    "a, b, c = TheOne(\"Comment\", True, 2)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"12\")\n",
    "a, b, c = TheOne(\"clean_Comment\", False, 2)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"13\")\n",
    "a, b, c = TheOne(\"No_Stop_Words_Comment\", False, 2)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"14\")\n",
    "a, b, c = TheOne(\"No_Stop_Words_Lemmatized_Comment\", False, 2)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])\n",
    "print(\"15\")\n",
    "a, b, c = TheOne(\"Comment\", False, 2)\n",
    "test_f1_list.append(a)\n",
    "cv_f1_list.append(b[\"test_f1_macro\"])\n",
    "event_list.append(c)\n",
    "cv_f1t_list.append(b[\"fit_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = {}\n",
    "tbl = []\n",
    "i = 1\n",
    "for x in range(1, 17):\n",
    "    ndf[x] = {\"Lemmatized\" : \"\", \"stop_words\" : \"\", \"binarized\" : \"\", \"n_gram\" : \"\", \"mean_f1\" : \"\", \"median_f1\" : \"\", \"std_f1\" : \"\", \"total_fit_time\" : \"\", \"test_f1\" : \"\", \"scenario\" : \"\"}\n",
    "for x in event_list:\n",
    "    new_list = []\n",
    "    if \"abstract\" == x[0]:\n",
    "        ndf[i][\"Lemmatized\"] = \"Not_Lemmatized\"\n",
    "        ndf[i][\"stop_words\"] = \"Included\"\n",
    "        new_list.append(\"Not_Lemmatized Stop Words Included\")\n",
    "    elif \"clean_abstract\" == x[0]:\n",
    "        ndf[i][\"Lemmatized\"] = \"Is_Lemmatized\"\n",
    "        ndf[i][\"stop_words\"] = \"Included\"\n",
    "        new_list.append(\"Is_Lemmatized Stop Words Included\")\n",
    "    elif \"No_Stop_Words_abstract\" == x[0]:\n",
    "        ndf[i][\"Lemmatized\"] = \"Not_Lemmatized\"\n",
    "        ndf[i][\"stop_words\"] = \"Not_Included\"\n",
    "        new_list.append(\"Not_Lemmatized Stop Words Not Included\")\n",
    "    else:\n",
    "        ndf[i][\"Lemmatized\"] = \"Is_Lemmatized\"\n",
    "        ndf[i][\"stop_words\"] = \"Not_Included\"\n",
    "        new_list.append(\"Is_Lemmatized Stop Words Not Included\")\n",
    "    if x[1] == \"Binary\":\n",
    "        ndf[i][\"binarized\"] = \"Yes\"\n",
    "        new_list.append(\"Is Binarized\")\n",
    "    else:\n",
    "        ndf[i][\"binarized\"] = \"No\"\n",
    "        new_list.append(\"Isn't Binarized\")\n",
    "    if x[2] == \"1_ngram\":\n",
    "        ndf[i][\"n_gram\"] = \"1\"\n",
    "        new_list.append(\"Is 1ngram\")\n",
    "    else:\n",
    "        ndf[i][\"n_gram\"] = \"2\"\n",
    "        new_list.append(\"Is 2ngram\")\n",
    "    tbl.append(\" \".join(new_list))\n",
    "    ndf[i][\"scenario\"] = \" \".join(new_list)\n",
    "    i = i + 1\n",
    "i = 1\n",
    "for x in test_f1_list:\n",
    "    ndf[i][\"test_f1\"] = x\n",
    "    i = i + 1\n",
    "i = 1\n",
    "for x in cv_f1t_list:\n",
    "    fn = 0\n",
    "    for y in x:\n",
    "        fn = y + fn\n",
    "    ndf[i][\"total_fit_time\"] = fn\n",
    "    i = i + 1\n",
    "i = 1\n",
    "for x in cv_f1_list:\n",
    "    ndf[i][\"mean_f1\"] = np.mean(x)\n",
    "    i = i + 1\n",
    "i = 1\n",
    "for x in cv_f1_list:\n",
    "    ndf[i][\"median_f1\"] = np.median(x)\n",
    "    i = i + 1\n",
    "i = 1\n",
    "for x in cv_f1_list:\n",
    "    ndf[i][\"std_f1\"] = np.std(x)\n",
    "    i = i + 1\n",
    "\n",
    "nndf = pd.DataFrame(ndf)\n",
    "nndf = pd.DataFrame.transpose(nndf)\n",
    "display(nndf)\n",
    "nndf.to_csv('model_card.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for x in cv_f1_list:\n",
    "    \n",
    "    data.append(x)\n",
    "fig = plt.figure(figsize =(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.boxplot(data)\n",
    "ax.set_xticklabels(tbl, rotation=90)\n",
    "i = 1\n",
    "\n",
    "plt.xlabel(\"Scenarios\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1 Scores by scenario\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleaning(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    noc = []\n",
    "    for y in x:\n",
    "        oa = y\n",
    "        na = nltk.regexp_tokenize(oa.lower(), r'(\\b[\\w]{2,}\\b)')\n",
    "        naa = []\n",
    "        for z in na:\n",
    "            naa.append(lemmatizer.lemmatize(z))\n",
    "        nnaa = \" \".join(naa)\n",
    "        noc.append(nnaa)\n",
    "    return noc\n",
    "\n",
    "pipeline = Pipeline([(\"Func\", FunctionTransformer(Cleaning)),(\"Vec\", CountVectorizer(ngram_range=(1, 2))), (\"Clf\", LogisticRegression())])\n",
    "pipeline.fit(X_train['Comment'], y_train)\n",
    "dill.settings['recurse'] = True\n",
    "dill.dump(pipeline, open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\"The Department of Veterans Affairs (VA) proposes to revise its regulations to\", 'health health health health', \"dog cat lamb\"]\n",
    "model_saved = dill.load(open('model.pkl','rb'))\n",
    "y_pred = model_saved.predict(test_strings)\n",
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
